# This protobuf is used to configure the train-adaptation

gpu: -1
seed: 17

init_model: ""
model_out: ""

# Currently this is not used
# discriminator_pretrained: ""

discriminator {
  hidden_units {
    feature_size: 256
    ngram: 3
  }
}

learning_config {
  optimizer {
    type: "adam"
    params {
      adam {
        alpha: 0.001
        beta1: 0.9
        beta2: 0.999
        eps: 0.00000001
      }
    }
  }
  dropout {
    percept_layer: 0.2
  }
  learning {
    method: "mrt"
    mrt {
      sharpness: 0.005
      sampling_frequency: 8
      generation_limit: 50
      eval_type: "discriminator"
    }
  }
  pretrain_epoch: 3
  seqgan_epoch: 3
  discriminator_epoch: 1
  generator_epoch: 1
}

corpus {
  in_domain_src {
    train_data {
      source: ""
      target: ""    
    }
    test_data {
      source: ""
      target: ""
    }
  }
  # Source should not be provided
  in_domain_trg {
    train_data {
      target: ""
    }
  }
}

data_config {
  batch: 1024
  batch_strategy: "word"
  max_sent_length: 50
  sort_method: "lentrg"
  max_item_in_batch: 256
}

mrt_data_config {
  batch: 128
  batch_strategy: "word"
  max_sent_length: 50
  sort_method: "lentrg"
  max_item_in_batch: 256
}

test_config {
  beam: 1
  generation_limit: 80
  word_penalty: 0.5
  evaluation {
    bleu {
      annotation: "bleu"
      ngram: 4
      smooth: 0
    }
    eval_ppl: true
  }
}

# Output-files
output_config {
  test {
    output_prefix: ""
    generate_attention: true
  }
}
