
import numpy
import six

import chainer
from chainer import cuda
from chainer import function
from chainer.utils import type_check

if cuda.cudnn_enabled:
    cudnn = cuda.cudnn
    libcudnn = cudnn.cudnn
    _algorithm = libcudnn.CUDNN_SOFTMAX_LOG
    _mode = libcudnn.CUDNN_SOFTMAX_MODE_CHANNEL
    _cudnn_version = libcudnn.getVersion()

class CrossEntropy(function.Function):

  """Softmax activation followed by a cross entropy loss."""

  ignore_label = -1

  def __init__(self, use_cudnn=True, normalize=True, cache_score=True):
    self.use_cudnn = use_cudnn
    self.normalize = normalize
    self.cache_score = cache_score

  def check_type_forward(self, in_types):
    type_check.expect(in_types.size() == 2)
    x_type, t_type = in_types

    type_check.expect(
      x_type.dtype == numpy.float32,
      t_type.dtype == numpy.int32,
      t_type.ndim == x_type.ndim - 1,

      x_type.shape[0] == t_type.shape[0],
      x_type.shape[2:] == t_type.shape[1:],
    )

  def _check_input_values(self, x, t):
    if not (((0 <= t) &
             (t < x.shape[1])) |
            (t == self.ignore_label)).all():
      msg = ('Each label `t` need to satisfy '
          '`0 <= t < x.shape[1] or t == %d`' % self.ignore_label)
      raise ValueError(msg)

  def forward_cpu(self, inputs):
    x, t = inputs
    if chainer.is_debug():
      self._check_input_values(x, t)
    
    log_y = numpy.log(x)
    if self.cache_score:
      self.y = x
    log_yd = numpy.rollaxis(log_y, 1)
    log_yd = log_yd.reshape(len(log_yd), -1)
    log_p = log_yd[numpy.maximum(t.ravel(), 0), six.moves.range(t.size)]
    if getattr(self, 'normalize', True):
      count = (t != self.ignore_label).sum()
    else:
      count = len(x)
    self._coeff = 1.0 / max(count, 1)
    y = (log_p * (t.ravel() != self.ignore_label)).sum(keepdims=True) * (-self._coeff)
    return y.reshape(()),

  def forward_gpu(self, inputs):
    cupy = cuda.cupy
    x, t = inputs
    if chainer.is_debug():
      self._check_input_values(x, t)

    log_y = cupy.log(x)
    if self.cache_score:
      self.y = x
    if getattr(self, 'normalize', True):
      coeff = cupy.maximum(1, (t != self.ignore_label).sum())
    else:
      coeff = max(1, len(t))
    self._coeff = cupy.divide(1.0, coeff, dtype=x.dtype)

    log_y = cupy.rollaxis(log_y, 1, log_y.ndim)
    ret = cuda.reduce(
      'S t, raw T log_y, int32 n_channel, raw T coeff', 'T out',
      't == -1 ? 0 : log_y[_j * n_channel + t]',
      'a + b', 'out = a * -coeff[0]', '0', 'crossent_fwd'
    )(t, log_y.reduced_view(), log_y.shape[-1], self._coeff)
    return ret,

  def backward_cpu(self, inputs, grad_outputs):
    x, t = inputs
    gloss = grad_outputs[0]
    if hasattr(self, 'y'):
      y = self.y.copy()
    else:
      y = x
    gx = y
    gv = numpy.zeros(x.shape, dtype=numpy.float32)
    gv[six.moves.xrange(len(t)), numpy.maximum(t, 0)] -= 1
    gx = numpy.divide(gv, gx)
    #gx *= (t != self.ignore_label).reshape((len(t), 1))

    gx *= gloss * self._coeff
    return gx, None

  def backward_gpu(self, inputs, grad_outputs):
    cupy = cuda.cupy
    x, t = inputs
    if hasattr(self, 'y'):
      y = self.y
    else:
      y = x
    gloss = grad_outputs[0]
    n_unit = t.size // len(t)
    coeff = gloss * self._coeff
    gx = cuda.elementwise(
      'T y, S t, raw T coeff, S n_channel, S n_unit',
      'T gx',
      '''
         const int c = (i / n_unit % n_channel);
         gx = (t == -1 || c != t) ? 0 : (coeff[0] * -1.0 / y);
      ''',
      'crossent_bwd')(
          y, cupy.expand_dims(t, 1), coeff, x.shape[1], n_unit)
    return gx, None

def cross_entropy(x, t, use_cudnn=True, normalize=True, cache_score=True):
  return CrossEntropy(use_cudnn, normalize, cache_score)(x, t)

